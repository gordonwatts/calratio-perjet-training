{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Decay Position Network\n",
    "\n",
    "This is repeating a little bit of the work that Rachel did - but going straight to a training. This notebook will use keras.\n",
    "\n",
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_adl_endpoint = 'http://localhost:8000'\n",
    "datasets_for_training_datafile = \"../data/datasets.csv\"\n",
    "\n",
    "# Constants derived in previous notebook. Need to be added to a python file of config constants.\n",
    "lxyz_eta_division = 1.3\n",
    "too_far_dist_lz = 7500\n",
    "too_far_dist_lxy = 4400\n",
    "too_short_dist_lxy=1300\n",
    "too_short_dist_lz=3500\n",
    "\n",
    "# How many events per training sample shall we train on?\n",
    "training_events_per_sample = 4000\n",
    "epochs_to_train = 250\n",
    "\n",
    "# Columns to train on. This is partly gotten by looking at the `Input Variables` worksheet to remove blanks.\n",
    "what_to_train_on = ['EMM_BL0', 'EMM_BL1', 'EMM_BL2',\n",
    "       'EMM_BL3', 'EMM_EL0', 'EMM_EL1', 'EMM_EL2', 'EMM_EL3', 'EH_EL0',\n",
    "       'EH_EL1', 'EH_EL2', 'EH_CBL0', 'EH_CBL1', 'EH_CVL2',\n",
    "       'EH_TGL0', 'EH_TGL1', 'EH_TGL2', 'EH_EBL0', 'EH_EBL1', 'EH_EBL2', 'JetPt', 'JetEta']\n",
    "# With an eta cut of 1.3, then EH_EL3 is also all zeros.\n",
    "#  'FC_L0', 'FC_L1', 'FC_L2' - these seem to be all zeros as seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Designed not to be modified\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from adl_func_client.event_dataset import EventDataset\n",
    "from adl_func_client.use_exe_func_adl_server import use_exe_func_adl_server\n",
    "from calratio_perjet_training.fetch_training_data import fetch_perjet_data\n",
    "import glob\n",
    "import numpy as np\n",
    "import asyncio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "from matplotlib import rcParams\n",
    "plt.rc('font', size=14)\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "#pd.set_option('display.max_rows', 500)\n",
    "#pd.set_option('display.max_columns', 500)\n",
    "#pd.set_option('display.width', 1000)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = pd.read_csv(datasets_for_training_datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_data_async(info):\n",
    "    return [info, f'{info.mH}_{info.mS}_{info.Lifetime}_{info.MCCampaign}', await fetch_perjet_data(EventDataset(f'localds://{info.RucioDSName}'), f'{info.mH}_{info.mS}_{info.Lifetime}_{info.MCCampaign}')]\n",
    "all_datasets_future = [fetch_data_async(info) for index, info in datasets.iterrows()]\n",
    "datasets_for_training = await asyncio.gather(*all_datasets_future)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to split into training and testing sample sizes. Unfortunately, we have to do some calculations to understand if something is good signal to train on, or not. So we add a few columns here to all the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_useful_columns(ds):\n",
    "    ds['Lxy'] = np.sqrt(ds.Lx*ds.Lx + ds.Ly*ds.Ly)\n",
    "\n",
    "    ds['IsOutlier'] = False\n",
    "    ds['IsOutlier'] |= ds.Lxy[ds.IsLLP & (np.abs(ds.JetEta) < lxyz_eta_division)] > too_far_dist_lxy\n",
    "    ds['IsOutlier'] |= ds.Lz[ds.IsLLP & (np.abs(ds.JetEta) >= lxyz_eta_division)] > too_far_dist_lz\n",
    "\n",
    "    ds['IsInlier'] = False\n",
    "    ds['IsInlier'] |= ds.Lxy[ds.IsLLP & (np.abs(ds.JetEta) < lxyz_eta_division)] < too_short_dist_lxy\n",
    "    ds['IsInlier'] |= ds.Lz[ds.IsLLP & (np.abs(ds.JetEta) >= lxyz_eta_division)] < too_short_dist_lz\n",
    "\n",
    "    ds['JetIsCentral'] = np.abs(ds.JetEta) < lxyz_eta_division\n",
    "\n",
    "    ds[\"Signal\"] = ds.IsLLP & (ds.JetPt > 40) & (np.abs(ds.JetEta) < 2.4) & (ds.IsOutlier == False) & (ds.IsInlier == False) & (ds.JetIsCentral == True)\n",
    "\n",
    "# We can limit how many datasets we combine to make life a little easier for testing with this line\n",
    "what_to_combine = datasets_for_training #[20:35]\n",
    "for d in what_to_combine:\n",
    "    add_useful_columns(d[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_training_jets = pd.DataFrame(pd.concat([d[2][d[2].Signal][:training_events_per_sample] for d in what_to_combine], keys=[(d[0].mH, d[0].mS, d[0].Lifetime, f'{d[0].mH}/{d[0].mS}', d[0].MCCampaign) for d in what_to_combine], names=['mH', 'mS', 'Lifetime', 'mH_mS', 'MC']).to_records())\n",
    "all_testing_jets = pd.DataFrame(pd.concat([d[2][d[2].Signal][training_events_per_sample+1:] for d in what_to_combine], keys=[(d[0].mH, d[0].mS, d[0].Lifetime, f'{d[0].mH}/{d[0].mS}', d[0].MCCampaign) for d in what_to_combine], names=['mH', 'mS', 'Lifetime', 'mH_mS', 'MC']).to_records())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f'Number of training jets: {len(all_training_jets)}')\n",
    "print (f'Number of testing jets: {len(all_testing_jets)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And quick reference for the columns we have in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_training_jets.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "For this training best to center thinngs around the average before doing the training (or inference). Create some tools to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_normalization (p):\n",
    "    return (p.mean(), p.std())\n",
    "\n",
    "(input_mean, input_std) = calc_normalization(all_training_jets.filter(items=what_to_train_on))\n",
    "(output_mean, output_std) = calc_normalization(all_training_jets.filter(items=['Lxy']))\n",
    "\n",
    "def norm_inputs(p):\n",
    "    return (p - input_mean) / input_std\n",
    "\n",
    "def norm_outputs(p):\n",
    "    return (p - output_mean[0]) / output_std[0]\n",
    "\n",
    "def unnorm_outputs(p):\n",
    "    return (p * output_std[0]) + output_mean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = norm_outputs(all_training_jets.Lxy)\n",
    "x_train = norm_inputs(all_training_jets.filter(items=what_to_train_on))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stdout --no-display\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=len(x_train.columns)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "# THis is the number of outputs - so could be 2 if we wanted to train both lxy and lz\n",
    "# Activation might be softmax if we had more than one thing as we would would want it to some to some number.\n",
    "# But since this is regression, we do not.\n",
    "model.add(Dense(1))\n",
    "\n",
    "#categorical_crossentropy\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=epochs_to_train, validation_split=0.25, shuffle=True, verbose=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "\n",
    "This is a little tricky in the sense we want to run the prediction for the whole table. We split out the testing stuff above. So normalize it and we are ready to go!\n",
    "Note we have to un-normalize things before we get to plotting and comparing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = norm_inputs(all_testing_jets.filter(items=what_to_train_on))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_testing_jets['p_Lxy'] = np.array(unnorm_outputs(y_predict[:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparisons\n",
    "\n",
    "Lets look at how well the prediction does vs various things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_testing_jets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=all_testing_jets.Lxy/1000.0, y=all_testing_jets.p_Lxy/1000.0)\n",
    "plt.xlabel('Actual value of $L_{xy}$')\n",
    "plt.ylabel('Predicted value of $L_{xy}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_comparison = sns.relplot(x='Lxy', y='p_Lxy', kind='scatter', data=all_testing_jets, row='mH', col='mS', height=6, aspect=1)\n",
    "mass_comparison.set(ylim=(0.0, 5000.0))\n",
    "mass_comparison.set(xlim=(0.0, 5000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "what_to_train_on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Calorimeter Edge Effect\n",
    "\n",
    "Looking at these plots there are many of them that have a seeming edge - a group of jets that have $L_{xy}$ that run along the calorimeter, but are all predicted to be at the face. My guess is this is connected with pileup somehow. So this is just an irreducable background.\n",
    "\n",
    "First, lets isolate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_testing_jets['EdgeBadReco'] = (all_testing_jets.p_Lxy < 1750) & (all_testing_jets.Lxy > 2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_comparison = sns.relplot(x='Lxy', y='p_Lxy', kind='scatter', hue='EdgeBadReco', data=all_testing_jets, height=6, aspect=1)\n",
    "mass_comparison.set(ylim=(0.0, 5000.0))\n",
    "mass_comparison.set(xlim=(0.0, 5000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_comparison = sns.relplot(x='Lxy', y='p_Lxy', kind='scatter', hue='EdgeBadReco', data=all_testing_jets, row='mH', col='mS', height=6, aspect=1)\n",
    "mass_comparison.set(ylim=(0.0, 5000.0))\n",
    "mass_comparison.set(xlim=(0.0, 5000.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kinematics?\n",
    "\n",
    "Get an idea behind simple kinematics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.distplot(x='JetPt', hue='EdgeBadReco', data=all_testing_jets)\n",
    "g = sns.FacetGrid(all_testing_jets[all_testing_jets.JetPt < 1000], col=\"mH\", hue='EdgeBadReco', height=5)\n",
    "g.map(sns.distplot, 'JetPt', norm_hist=False, kde=False)\n",
    "g.set(yscale='log')\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.distplot(x='JetPt', hue='EdgeBadReco', data=all_testing_jets)\n",
    "g = sns.FacetGrid(all_testing_jets[all_testing_jets.JetPt < 1000], col=\"mH\", hue='EdgeBadReco', height=5)\n",
    "g.map(sns.distplot, 'JetEta', norm_hist=False, kde=False)\n",
    "g.set(yscale='log')\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.distplot(x='JetPt', hue='EdgeBadReco', data=all_testing_jets)\n",
    "g = sns.FacetGrid(all_testing_jets[all_testing_jets.JetPt < 1000], col=\"mH\", hue='EdgeBadReco', height=5)\n",
    "g.map(sns.distplot, 'JetPhi', norm_hist=False, kde=False)\n",
    "g.set(yscale='log')\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.distplot(x='JetPt', hue='EdgeBadReco', data=all_testing_jets)\n",
    "g = sns.FacetGrid(all_testing_jets[all_testing_jets.JetPt < 1000], col=\"mH\", hue='EdgeBadReco', height=5)\n",
    "g.map(plt.scatter, 'JetEta', 'JetPhi')\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the inner layer of the EM. We know from the input variables plot that it is not really responding for an $L_{xy} > 1750$ mm. So that means that we should see little energy there. If there is another jet - like a pileup jet, then we should see an energy deposit there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.distplot(x='JetPt', hue='EdgeBadReco', data=all_testing_jets)\n",
    "g = sns.FacetGrid(all_testing_jets[(all_testing_jets.JetPt < 1000) & (all_testing_jets.Lxy > 1750)], col=\"mH\", hue='EdgeBadReco', height=5)\n",
    "g.map(sns.distplot, 'EMM_BL0', norm_hist=False, kde=False)\n",
    "g.set(yscale='log')\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I admit to being a bit suprised. This is inconclusive:\n",
    "\n",
    "- On one hand, it the well reconstructed often has lower energy than this guy.\n",
    "- But that isn't always the case!\n",
    "\n",
    "We can definately conclude that EMM_BL0 is higher for these funny jets. But it isn't black & white."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
