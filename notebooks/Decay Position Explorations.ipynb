{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Decay Position Network\n",
    "\n",
    "This is repeating a little bit of the work that Rachel did - but going straight to a training. This notebook will use keras.\n",
    "\n",
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_adl_endpoint = 'http://localhost:31000'\n",
    "datasets_for_training_datafile = \"../data/datasets.csv\"\n",
    "\n",
    "# Constants derived in previous notebook. Need to be added to a python file of config constants.\n",
    "lxyz_eta_division = 1.3\n",
    "too_far_dist_lz = 7500\n",
    "too_far_dist_lxy = 4400\n",
    "too_short_dist_lxy=1300\n",
    "too_short_dist_lz=3500\n",
    "\n",
    "# How many events per training sample shall we train on?\n",
    "training_events_per_sample = 4000\n",
    "epochs_to_train = 100\n",
    "\n",
    "# Columns to train on. This is partly gotten by looking at the `Input Variables` worksheet to remove blanks.\n",
    "what_to_train_on = ['EMM_BL0', 'EMM_BL1', 'EMM_BL2',\n",
    "       'EMM_BL3', 'EMM_EL0', 'EMM_EL1', 'EMM_EL2', 'EMM_EL3', 'EH_EL0',\n",
    "       'EH_EL1', 'EH_EL2', 'EH_CBL0', 'EH_CBL1', 'EH_CVL2',\n",
    "       'EH_TGL0', 'EH_TGL1', 'EH_TGL2', 'EH_EBL0', 'EH_EBL1', 'EH_EBL2', 'JetPt', 'JetEta']\n",
    "# With an eta cut of 1.3, then EH_EL3 is also all zeros.\n",
    "#  'FC_L0', 'FC_L1', 'FC_L2' - these seem to be all zeros as seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Designed not to be modified\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from func_adl import EventDataset\n",
    "from func_adl.xAOD import FuncADLServerException\n",
    "\n",
    "from calratio_perjet_training.fetch_training_data import fetch_perjet_data\n",
    "import glob\n",
    "import numpy as np\n",
    "import asyncio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "from matplotlib import rcParams\n",
    "plt.rc('font', size=14)\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "#pd.set_option('display.max_rows', 500)\n",
    "#pd.set_option('display.max_columns', 500)\n",
    "#pd.set_option('display.width', 1000)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "datasets = pd.read_csv(datasets_for_training_datafile).query('Use==1')[10:11]\n",
    "print(len(datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 125, 5, 9, mc16d\n",
      "  Done with 125, 5, 9, mc16d\n"
     ]
    }
   ],
   "source": [
    "async def fetch_data_async(info):\n",
    "    try:\n",
    "        print(f'Starting {info.mH}, {info.mS}, {info.Lifetime}, {info.MCCampaign}')\n",
    "        d = await fetch_perjet_data(EventDataset(f'cacheds://{info.RucioDSName}'), f'{info.mH}_{info.mS}_{info.Lifetime}_{info.MCCampaign}', endpoint=func_adl_endpoint, jet_pt_cut=40.0, deltar_llp=0.6)\n",
    "        print(f'  Done with {info.mH}, {info.mS}, {info.Lifetime}, {info.MCCampaign}')\n",
    "        return [info, f'{info.mH}_{info.mS}_{info.Lifetime}_{info.MCCampaign}', d]\n",
    "    except FuncADLServerException as e:\n",
    "        print (str(e))\n",
    "        return None\n",
    "all_datasets_future = [fetch_data_async(info) for index, info in datasets.iterrows()]\n",
    "datasets_for_training = [f for f in (await asyncio.gather(*all_datasets_future)) if f is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[mH                                                           125\n",
       "  mS                                                             5\n",
       "  Lifetime                                                       9\n",
       "  MCCampaign                                                 mc16d\n",
       "  RucioDSName    mc16_13TeV.311310.MadGraphPythia8EvtGen_A14NNP...\n",
       "  Use                                                            1\n",
       "  Comments                                                     NaN\n",
       "  Name: 10, dtype: object, '125_5_9_mc16d', Empty DataFrame\n",
       "  Columns: [RunNumber, EventNumber, JetPt, JetEta, JetPhi, IsLLP, nLLPs_Near_Jets, Lx, Ly, Lz, Leta, Lphi, Lpt, Lpz, EMM_BL0, EMM_BL1, EMM_BL2, EMM_BL3, EMM_EL0, EMM_EL1, EMM_EL2, EMM_EL3, EH_EL0, EH_EL1, EH_EL2, EH_EL3, EH_CBL0, EH_CBL1, EH_CVL2, EH_TGL0, EH_TGL1, EH_TGL2, EH_EBL0, EH_EBL1, EH_EBL2, FC_L0, FC_L1, FC_L2]\n",
       "  Index: []\n",
       "  \n",
       "  [0 rows x 38 columns]]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_for_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to split into training and testing sample sizes. Unfortunately, we have to do some calculations to understand if something is good signal to train on, or not. So we add a few columns here to all the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_useful_columns(ds):\n",
    "    ds['Lxy'] = np.sqrt(ds.Lx*ds.Lx + ds.Ly*ds.Ly)\n",
    "\n",
    "    ds['IsOutlier'] = False\n",
    "    ds['IsOutlier'] |= ds.Lxy[ds.IsLLP & (np.abs(ds.JetEta) < lxyz_eta_division)] > too_far_dist_lxy\n",
    "    ds['IsOutlier'] |= ds.Lz[ds.IsLLP & (np.abs(ds.JetEta) >= lxyz_eta_division)] > too_far_dist_lz\n",
    "\n",
    "    ds['IsInlier'] = False\n",
    "    ds['IsInlier'] |= ds.Lxy[ds.IsLLP & (np.abs(ds.JetEta) < lxyz_eta_division)] < too_short_dist_lxy\n",
    "    ds['IsInlier'] |= ds.Lz[ds.IsLLP & (np.abs(ds.JetEta) >= lxyz_eta_division)] < too_short_dist_lz\n",
    "\n",
    "    ds['JetIsCentral'] = np.abs(ds.JetEta) < lxyz_eta_division\n",
    "\n",
    "    ds[\"Signal\"] = ds.IsLLP & (ds.JetPt > 40) & (np.abs(ds.JetEta) < 2.4) & (ds.IsOutlier == False) & (ds.IsInlier == False) & (ds.JetIsCentral == True)\n",
    "\n",
    "# We can limit how many datasets we combine to make life a little easier for testing with this line\n",
    "what_to_combine = datasets_for_training #[20:35]\n",
    "for d in what_to_combine:\n",
    "    add_useful_columns(d[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 21,\n",
       " 0,\n",
       " 0,\n",
       " 29,\n",
       " 0,\n",
       " 30,\n",
       " 0,\n",
       " 20,\n",
       " 0,\n",
       " 25,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 15,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(ds[2]) for ds in what_to_combine]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'names', 'formats', 'offsets', and 'titles' dict entries must have the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-eced8ad1e088>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mall_training_jets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSignal\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtraining_events_per_sample\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwhat_to_combine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLifetime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf'{d[0].mH}/{d[0].mS}'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMCCampaign\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwhat_to_combine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mH'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'mS'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Lifetime'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'mH_mS'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'MC'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mall_testing_jets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSignal\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtraining_events_per_sample\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwhat_to_combine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLifetime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf'{d[0].mH}/{d[0].mS}'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMCCampaign\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwhat_to_combine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mH'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'mS'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Lifetime'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'mH_mS'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'MC'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mto_records\u001b[1;34m(self, index, convert_datetime64, column_dtypes, index_dtypes)\u001b[0m\n\u001b[0;32m   1817\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1819\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromarrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"names\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"formats\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mformats\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1821\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\records.py\u001b[0m in \u001b[0;36mfromarrays\u001b[1;34m(arrayList, dtype, shape, formats, names, titles, aligned, byteorder)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 608\u001b[1;33m         \u001b[0mdescr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    609\u001b[0m         \u001b[0m_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdescr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: 'names', 'formats', 'offsets', and 'titles' dict entries must have the same length"
     ]
    }
   ],
   "source": [
    "all_training_jets = pd.DataFrame(pd.concat([d[2][d[2].Signal][:training_events_per_sample] for d in what_to_combine], keys=[(d[0].mH, d[0].mS, d[0].Lifetime, f'{d[0].mH}/{d[0].mS}', d[0].MCCampaign) for d in what_to_combine], names=['mH', 'mS', 'Lifetime', 'mH_mS', 'MC']).to_records())\n",
    "all_testing_jets = pd.DataFrame(pd.concat([d[2][d[2].Signal][training_events_per_sample+1:] for d in what_to_combine], keys=[(d[0].mH, d[0].mS, d[0].Lifetime, f'{d[0].mH}/{d[0].mS}', d[0].MCCampaign) for d in what_to_combine], names=['mH', 'mS', 'Lifetime', 'mH_mS', 'MC']).to_records())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f'Number of training jets: {len(all_training_jets)}')\n",
    "print (f'Number of testing jets: {len(all_testing_jets)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And quick reference for the columns we have in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_training_jets.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "For this training best to center thinngs around the average before doing the training (or inference). Create some tools to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_normalization (p):\n",
    "    return (p.mean(), p.std())\n",
    "\n",
    "(input_mean, input_std) = calc_normalization(all_training_jets.filter(items=what_to_train_on))\n",
    "(output_mean, output_std) = calc_normalization(all_training_jets.filter(items=['Lxy']))\n",
    "\n",
    "def norm_inputs(p):\n",
    "    return (p - input_mean) / input_std\n",
    "\n",
    "def norm_outputs(p):\n",
    "    return (p - output_mean[0]) / output_std[0]\n",
    "\n",
    "def unnorm_outputs(p):\n",
    "    return (p * output_std[0]) + output_mean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = norm_outputs(all_training_jets.Lxy)\n",
    "x_train = norm_inputs(all_training_jets.filter(items=what_to_train_on))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stdout --no-display\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=len(x_train.columns)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "# THis is the number of outputs - so could be 2 if we wanted to train both lxy and lz\n",
    "# Activation might be softmax if we had more than one thing as we would would want it to some to some number.\n",
    "# But since this is regression, we do not.\n",
    "model.add(Dense(1))\n",
    "\n",
    "#categorical_crossentropy\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=epochs_to_train, validation_split=0.25, shuffle=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "\n",
    "This is a little tricky in the sense we want to run the prediction for the whole table. We split out the testing stuff above. So normalize it and we are ready to go!\n",
    "Note we have to un-normalize things before we get to plotting and comparing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = norm_inputs(all_testing_jets.filter(items=what_to_train_on))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_testing_jets['p_Lxy'] = np.array(unnorm_outputs(y_predict[:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparisons\n",
    "\n",
    "Lets look at how well the prediction does vs various things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_testing_jets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=all_testing_jets.Lxy/1000.0, y=all_testing_jets.p_Lxy/1000.0)\n",
    "plt.xlabel('Actual value of $L_{xy}$')\n",
    "plt.ylabel('Predicted value of $L_{xy}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_comparison = sns.relplot(x='Lxy', y='p_Lxy', kind='scatter', data=all_testing_jets, row='mH', col='mS', height=6, aspect=1)\n",
    "mass_comparison.set(ylim=(0.0, 5000.0))\n",
    "mass_comparison.set(xlim=(0.0, 5000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "what_to_train_on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Calorimeter Edge Effect\n",
    "\n",
    "Looking at these plots there are many of them that have a seeming edge - a group of jets that have $L_{xy}$ that run along the calorimeter, but are all predicted to be at the face. My guess is this is connected with pileup somehow. So this is just an irreducable background.\n",
    "\n",
    "First, lets isolate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_testing_jets['EdgeBadReco'] = (all_testing_jets.p_Lxy < 1750) & (all_testing_jets.Lxy > 2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_comparison = sns.relplot(x='Lxy', y='p_Lxy', kind='scatter', hue='EdgeBadReco', data=all_testing_jets, height=6, aspect=1)\n",
    "mass_comparison.set(ylim=(0.0, 5000.0))\n",
    "mass_comparison.set(xlim=(0.0, 5000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"white\"):\n",
    "    sns.jointplot(x='Lxy', y='p_Lxy', kind=\"hex\", color=\"k\", height=6, data=all_testing_jets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_comparison = sns.relplot(x='Lxy', y='p_Lxy', kind='scatter', hue='EdgeBadReco', data=all_testing_jets, row='mH', col='mS', height=6, aspect=1)\n",
    "mass_comparison.set(ylim=(0.0, 5000.0))\n",
    "mass_comparison.set(xlim=(0.0, 5000.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kinematics?\n",
    "\n",
    "Get an idea behind simple kinematics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.distplot(x='JetPt', hue='EdgeBadReco', data=all_testing_jets)\n",
    "g = sns.FacetGrid(all_testing_jets[all_testing_jets.JetPt < 1000], col=\"mH\", hue='EdgeBadReco', height=5)\n",
    "g.map(sns.distplot, 'JetPt', norm_hist=False, kde=False)\n",
    "g.set(yscale='log')\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(all_testing_jets[(all_testing_jets.JetPt < 1000) & (all_testing_jets.EdgeBadReco==True)], hue='mH', height=7, aspect=2)\n",
    "g.map(sns.distplot, 'JetPt', norm_hist=False, kde=False)\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.distplot(x='JetPt', hue='EdgeBadReco', data=all_testing_jets)\n",
    "g = sns.FacetGrid(all_testing_jets[all_testing_jets.JetPt < 1000], col=\"mH\", hue='EdgeBadReco', height=5)\n",
    "g.map(sns.distplot, 'JetEta', norm_hist=False, kde=False)\n",
    "g.set(yscale='log')\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.distplot(x='JetPt', hue='EdgeBadReco', data=all_testing_jets)\n",
    "g = sns.FacetGrid(all_testing_jets[all_testing_jets.JetPt < 1000], col=\"mH\", hue='EdgeBadReco', height=5)\n",
    "g.map(sns.distplot, 'JetPhi', norm_hist=False, kde=False)\n",
    "g.set(yscale='log')\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.distplot(x='JetPt', hue='EdgeBadReco', data=all_testing_jets)\n",
    "g = sns.FacetGrid(all_testing_jets[all_testing_jets.JetPt < 1000], col=\"mH\", hue='EdgeBadReco', height=5)\n",
    "g.map(plt.scatter, 'JetEta', 'JetPhi')\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the inner layer of the EM. We know from the input variables plot that it is not really responding for an $L_{xy} > 1750$ mm. So that means that we should see little energy there. If there is another jet - like a pileup jet, then we should see an energy deposit there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.distplot(x='JetPt', hue='EdgeBadReco', data=all_testing_jets)\n",
    "g = sns.FacetGrid(all_testing_jets[(all_testing_jets.JetPt < 1000) & (all_testing_jets.Lxy > 1750)], col=\"mH\", hue='EdgeBadReco', height=5)\n",
    "g.map(sns.distplot, 'EMM_BL0', norm_hist=False, kde=False)\n",
    "g.set(yscale='log')\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_comparison = sns.relplot(x='Lxy', y='p_Lxy', kind='scatter', hue='EMM_BL0', data=all_testing_jets, height=6, aspect=1)\n",
    "mass_comparison.set(ylim=(0.0, 5000.0))\n",
    "mass_comparison.set(xlim=(0.0, 5000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_comparison = sns.relplot(x='Lxy', y='p_Lxy', kind='scatter', hue='Lz', data=all_testing_jets, height=6, aspect=1)\n",
    "mass_comparison.set(ylim=(1000.0, 5000.0))\n",
    "mass_comparison.set(xlim=(1000.0, 5000.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these look like they have very very high $L_z$ values and also high $L_xy$ - which doesn't make sense from the POV of $|\\eta|<1.3$ cut. Lets quickly look at a few of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_testing_jets[(all_testing_jets.EdgeBadReco==True) & (all_testing_jets.Lz > 9000.0)].filter(items=['JetEta', 'Lxy', 'Lz'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm... first of all - repeated events. That isn't good! Second, the error was looking at $p_L_{xy}$, not $L_{xy}$. Ok that makes lots more sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I admit to being a bit suprised. This 2D plot is inconclusive:\n",
    "\n",
    "- On one hand, it the well reconstructed often has lower energy than this guy.\n",
    "- But that isn't always the case!\n",
    "\n",
    "We can definately conclude that EMM_BL0 is higher for these funny jets. But it isn't black & white."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the difference in $p_T$ between the LLP and the jet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_testing_jets['DeltaMCPt'] = np.abs(all_testing_jets.JetPt - all_testing_jets.Lpt)/all_testing_jets.Lpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(all_testing_jets[all_testing_jets.JetPt < 1000], col=\"mH\", hue='EdgeBadReco', height=5)\n",
    "g.map(sns.distplot, 'DeltaMCPt', norm_hist=False, kde=False)\n",
    "g.set(yscale='log')\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_testing_jets['DeltaMCPtRaw'] = (all_testing_jets.JetPt - all_testing_jets.Lpt)/all_testing_jets.Lpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(all_testing_jets[all_testing_jets.JetPt < 1000], col=\"mH\", hue='EdgeBadReco', height=5)\n",
    "g.map(sns.distplot, 'DeltaMCPtRaw', norm_hist=False, kde=False)\n",
    "g.set(yscale='log')\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
